{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from env import HighwayEnv, convert_highd_sample_to_gail_expert\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayEnvBuffer:\n",
    "    def __init__(self, h_dim, v_dim, f_dim, act_dim, m_dim=10, b_dim=2, size=800, gamma=0.99, lam=0.95, device='cuda'):\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.kn_buf = torch.zeros((size, h_dim, v_dim, f_dim))\n",
    "        self.lm_buf = torch.zeros((size, m_dim))\n",
    "        self.bd_buf = torch.zeros((size, b_dim))\n",
    "        self.mk_buf = torch.zeros((size, v_dim))\n",
    "        self.act_buf = torch.zeros((size, v_dim, act_dim))\n",
    "        self.logp_buf = torch.zeros((size, v_dim))\n",
    "        self.rew_buf = torch.zeros(size)\n",
    "        self.val_buf = torch.zeros((size, v_dim))\n",
    "        self.adv_buf = torch.zeros(size)\n",
    "        self.ret_buf = torch.zeros(size)\n",
    "        self.ptr = 0\n",
    "        self.path_start_idx = 0\n",
    "        self.max_size = size\n",
    "\n",
    "    def store(self, kn, lm, bd, mk, act, logp, rew, val):\n",
    "        assert self.ptr < self.max_size\n",
    "        self.kn_buf[self.ptr]  = kn\n",
    "        self.lm_buf[self.ptr]  = lm\n",
    "        self.bd_buf[self.ptr]  = bd\n",
    "        self.mk_buf[self.ptr]   = mk\n",
    "        self.act_buf[self.ptr]  = act\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.rew_buf[self.ptr]  = rew\n",
    "        self.val_buf[self.ptr]  = val\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val):\n",
    "        \"\"\"\n",
    "        last_val: (M,)  value estimates for step after the end\n",
    "        \"\"\"\n",
    "        slice_ = slice(self.path_start_idx, self.ptr)\n",
    "        # append last_val to compute deltas\n",
    "        rews = torch.cat([self.rew_buf[slice_], last_val.unsqueeze(0)], dim=0)  # (L+1, M)\n",
    "        vals = torch.cat([self.val_buf[slice_], last_val.unsqueeze(0)], dim=0)  # (L+1, M)\n",
    "        # GAE deltas\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]                  # (L, M)\n",
    "\n",
    "        # compute advantage\n",
    "        adv = torch.zeros_like(deltas)\n",
    "        gae = torch.zeros(self.M)\n",
    "        for t in reversed(range(deltas.shape[0])):\n",
    "            gae = deltas[t] + self.gamma * self.lam * gae\n",
    "            adv[t] = gae\n",
    "        self.adv_buf[slice_] = adv\n",
    "\n",
    "        # rewards-to-go\n",
    "        ret = torch.zeros_like(rews)\n",
    "        ret[-1] = last_val\n",
    "        for t in reversed(range(rews.shape[0]-1)):\n",
    "            ret[t] = rews[t] + self.gamma * ret[t+1]\n",
    "        self.ret_buf[slice_] = ret[:-1]\n",
    "\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size  # buffer full\n",
    "        # normalize advantages\n",
    "        adv_mean, adv_std = self.adv_buf.mean(), self.adv_buf.std() + 1e-8\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(kn=self.kn_buf,\n",
    "                    lm=self.lm_buf,\n",
    "                    bd=self.bd_buf,\n",
    "                    mk=self.mk_buf,\n",
    "                    act=self.act_buf,\n",
    "                    logp=self.logp_buf,\n",
    "                    ret=self.ret_buf,\n",
    "                    adv=self.adv_buf)\n",
    "        # reset pointer\n",
    "        self.ptr = 0\n",
    "        self.path_start_idx = 0\n",
    "        return {k: v for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seq_lengths(time_dep):\n",
    "    \"\"\"\n",
    "    Given time_dep of shape (B, T, F), return a tensor of sequence lengths,\n",
    "    where a timestep is considered valid if not all features are NaN.\n",
    "    \"\"\"\n",
    "    # valid if not all F values are NaN:\n",
    "    valid_mask = ~torch.all(torch.isnan(time_dep), dim=-1)  # shape (B, T)\n",
    "    seq_lengths = valid_mask.sum(dim=1)  # (B,)\n",
    "    seq_lengths[seq_lengths == 0] = 1  # avoid zeros\n",
    "    return seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Actor Network (Policy)\n",
    "#########################################\n",
    "class PPOActor(nn.Module):\n",
    "    def __init__(self, h_dim, v_dim, f_dim, lstm_hidden=64, global_dim=12, combined_hidden=64, output_size=2):\n",
    "        \"\"\"\n",
    "        Actor network that takes the structured observation and outputs\n",
    "        a Gaussian distribution over acceleration commands.\n",
    "        \n",
    "        Observation:\n",
    "           - time_dep: (N, T, M, F) \n",
    "           - lane_markers: (N, 10)\n",
    "           - boundary_lines: (N, 2)\n",
    "        Output:\n",
    "           - accelerations: (N, M, 2)\n",
    "        \"\"\"\n",
    "        super(PPOActor, self).__init__()\n",
    "        self.T = h_dim\n",
    "        self.M = v_dim\n",
    "        self.F = f_dim\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        \n",
    "        # LSTM for time-dependent kinematics (for each vehicle's sequence of length T and F features).\n",
    "        self.lstm = nn.LSTM(input_size=self.F, hidden_size=lstm_hidden, batch_first=True)\n",
    "        \n",
    "        # Global information: process lane markers and boundaries separately.\n",
    "        self.global_fc = nn.Linear(10, global_dim)  # lane markers of shape (N, 10)\n",
    "        self.boundary_fc = nn.Linear(2, global_dim)   # boundaries of shape (N, 2)\n",
    "        self.global_combine_fc = nn.Linear(2 * global_dim, global_dim)\n",
    "        \n",
    "        # Fully connected layers that combine per-vehicle features with global info.\n",
    "        self.combine_fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + global_dim, combined_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        # Learnable log standard deviation for the Gaussian distribution over actions.\n",
    "        # We use a parameter of shape (1, 1, output_size) that will be broadcast.\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, 1, output_size))\n",
    "        \n",
    "    def forward(self, time_dep, lane_markers, boundaries):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "           time_dep: (N, T, M, F)\n",
    "           lane_markers: (N, 10)  (may contain NaNs)\n",
    "           boundaries: (N, 2)\n",
    "        \n",
    "        Returns:\n",
    "           mean: (N, M, 2) — the mean acceleration for each vehicle slot.\n",
    "           log_std: (N, M, 2) — the log_std, broadcasted along N and M.\n",
    "        \"\"\"\n",
    "        if time_dep.dim() == 3:\n",
    "            time_dep = time_dep.unsqueeze(0) \n",
    "        if lane_markers.dim() == 1:\n",
    "            lane_markers = lane_markers.unsqueeze(0) \n",
    "        if boundaries.dim() == 1:\n",
    "            boundaries = boundaries.unsqueeze(0) \n",
    "\n",
    "        N, T, M, _F = time_dep.shape\n",
    "        \n",
    "        # Process time-dependent kinematics:\n",
    "        # Permute to (N, M, T, F) then flatten N and M to get (N*M, T, F)\n",
    "        time_dep = time_dep.permute(0, 2, 1, 3).contiguous().view(N * M, T, _F)\n",
    "        # Replace NaNs with 0:\n",
    "        time_dep_clean = torch.nan_to_num(time_dep, nan=0.0)\n",
    "        # Compute sequence lengths per vehicle:\n",
    "        seq_lengths = compute_seq_lengths(time_dep)\n",
    "        assert not torch.isnan(time_dep_clean).any()\n",
    "        # Pack padded sequence:\n",
    "        packed = rnn_utils.pack_padded_sequence(time_dep_clean, lengths=seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "        # hn: (1, N*M, lstm_hidden) -> squeeze to (N*M, lstm_hidden)\n",
    "        hn = hn.squeeze(0)\n",
    "        # Reshape back to (N, M, lstm_hidden)\n",
    "        vehicle_repr = hn.view(N, M, self.lstm_hidden)\n",
    "        \n",
    "        # Process global inputs:\n",
    "        # For lane markers, replace NaNs with 0 and create a mask.\n",
    "        lane_markers_clean = torch.nan_to_num(lane_markers, nan=0.0)  # (N, 10)\n",
    "        lane_mask = (~torch.isnan(lane_markers)).float()  # (N, 10)\n",
    "        global_lane_features = F.relu(self.global_fc(lane_markers_clean))  # (N, global_dim)\n",
    "        valid_ratio = lane_mask.mean(dim=1, keepdim=True)  # (N, 1)\n",
    "        global_lane_features = global_lane_features * valid_ratio\n",
    "        \n",
    "        global_boundaries_features = F.relu(self.boundary_fc(boundaries))  # (N, global_dim)\n",
    "        global_combined = torch.cat([global_lane_features, global_boundaries_features], dim=1)  # (N, 2*global_dim)\n",
    "        global_info = F.relu(self.global_combine_fc(global_combined))  # (N, global_dim)\n",
    "        global_info = global_info.unsqueeze(1).expand(-1, M, -1)  # (N, M, global_dim)\n",
    "        \n",
    "        # Combine vehicle representation with global info:\n",
    "        combined = torch.cat([vehicle_repr, global_info], dim=-1)  # (N, M, lstm_hidden + global_dim)\n",
    "        mean = self.combine_fc(combined)  # (N, M, output_size)\n",
    "        \n",
    "        # Broadcast log_std:\n",
    "        log_std = self.log_std.expand(N, M, -1)  # (N, M, output_size)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def get_action(self, time_dep, lane_markers, boundaries, agent_mask):\n",
    "        \"\"\"\n",
    "        Sample actions from the policy distribution.\n",
    "        \n",
    "        Returns:\n",
    "           action: (N, M, output_size)\n",
    "           log_prob: (N, M, output_size) or summed over output_size per vehicle.\n",
    "        \"\"\"\n",
    "        _agent_mask = agent_mask.unsqueeze(0) if agent_mask.dim() == 1 else agent_mask\n",
    "        mean, log_std = self.forward(time_dep, lane_markers, boundaries)\n",
    "        # print (mean, log_std)\n",
    "        std = torch.exp(log_std)\n",
    "        # Create a normal distribution per vehicle slot.\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        # Sample actions using reparameterization (this allows for differentiable sampling).\n",
    "        action = dist.rsample()  # shape (N, M, output_size)\n",
    "        # Compute log probabilities.\n",
    "        log_prob = dist.log_prob(action)  # shape (N, M, output_size)\n",
    "        log_prob = log_prob.sum(dim=-1)  # aggregate over action dimensions, shape (N, M)\n",
    "        # Mask out non-agent slots: set log_prob to 0 for non-agent vehicles.\n",
    "        # This means that when computing the loss, only entries with agent_mask==1 will contribute.\n",
    "        masked_log_prob = log_prob * _agent_mask  # agent_mask is assumed to be float, with 1 or 0.\n",
    "        masked_action = action * _agent_mask.unsqueeze(-1) \n",
    "        \n",
    "        return masked_action, masked_log_prob\n",
    "    \n",
    "#########################################\n",
    "# Critic Network (Value Function)\n",
    "#########################################\n",
    "class PPOCritic(nn.Module):\n",
    "    def __init__(self, h_dim, v_dim, f_dim, lstm_hidden=64, global_dim=12, combined_hidden=64):\n",
    "        \"\"\"\n",
    "        Critic network that takes the same observation and outputs a scalar value for each sample.\n",
    "        \n",
    "        In many cases we want a state value per sample (N,1) that is derived from the entire observation.\n",
    "        We can process each vehicle's time-dependent sequence similar to the actor,\n",
    "        combine with global features, and then aggregate (for instance, by averaging over vehicles).\n",
    "        \"\"\"\n",
    "        super(PPOCritic, self).__init__()\n",
    "        self.T = h_dim\n",
    "        self.M = v_dim\n",
    "        self.F = f_dim\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        \n",
    "        # LSTM for time-dependent kinematics (per vehicle):\n",
    "        self.lstm = nn.LSTM(input_size=self.F, hidden_size=lstm_hidden, batch_first=True)\n",
    "        \n",
    "        # Global information processing:\n",
    "        self.global_fc = nn.Linear(10, global_dim)  # for lane markers\n",
    "        self.boundary_fc = nn.Linear(2, global_dim)   # for boundaries\n",
    "        self.global_combine_fc = nn.Linear(2 * global_dim, global_dim)\n",
    "        \n",
    "        # Combine per-vehicle features with global info and aggregate:\n",
    "        # We combine each vehicle's representation and then average over vehicles.\n",
    "        self.combine_fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + global_dim, combined_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_hidden, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, time_dep, lane_markers, boundaries, agent_mask):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "           time_dep: (N, T, M, F)\n",
    "           lane_markers: (N, 10)\n",
    "           boundaries: (N, 2)\n",
    "           agent_mask: (N, M) binary mask for valid vehicles.\n",
    "           \n",
    "        Returns:\n",
    "           values: (N, 1) scalar state-value estimates.\n",
    "        \"\"\"\n",
    "        if time_dep.dim() == 3:\n",
    "            time_dep = time_dep.unsqueeze(0) \n",
    "        if lane_markers.dim() == 1:\n",
    "            lane_markers = lane_markers.unsqueeze(0) \n",
    "        if boundaries.dim() == 1:\n",
    "            boundaries = boundaries.unsqueeze(0) \n",
    "        N, T, M, _F = time_dep.shape\n",
    "        \n",
    "        # Process time-dependent input as before:\n",
    "        time_dep = time_dep.permute(0, 2, 1, 3).contiguous().view(N * M, T, _F)\n",
    "        time_dep_clean = torch.nan_to_num(time_dep, nan=0.0)\n",
    "        valid_mask = ~torch.all(torch.isnan(time_dep), dim=-1)  # (N*M, T)\n",
    "        seq_lengths = valid_mask.sum(dim=1)\n",
    "        seq_lengths[seq_lengths == 0] = 1\n",
    "        packed = rnn_utils.pack_padded_sequence(time_dep_clean, lengths=seq_lengths.cpu(),\n",
    "                                                  batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "        hn = hn.squeeze(0)  # (N*M, lstm_hidden)\n",
    "        vehicle_repr = hn.view(N, M, self.lstm_hidden)\n",
    "        \n",
    "        # Process global inputs:\n",
    "        lane_markers_clean = torch.nan_to_num(lane_markers, nan=0.0)\n",
    "        lane_mask = (~torch.isnan(lane_markers)).float()\n",
    "        global_lane_features = F.relu(self.global_fc(lane_markers_clean))  # (N, global_dim)\n",
    "        valid_ratio = lane_mask.mean(dim=1, keepdim=True)\n",
    "        global_lane_features = global_lane_features * valid_ratio\n",
    "        \n",
    "        global_boundaries_features = F.relu(self.boundary_fc(boundaries))  # (N, global_dim)\n",
    "        global_combined = torch.cat([global_lane_features, global_boundaries_features], dim=1)\n",
    "        global_info = F.relu(self.global_combine_fc(global_combined))  # (N, global_dim)\n",
    "        global_info = global_info.unsqueeze(1).expand(-1, M, -1)  # (N, M, global_dim)\n",
    "        \n",
    "        # Combine vehicle representation and global features.\n",
    "        combined = torch.cat([vehicle_repr, global_info], dim=-1)  # (N, M, lstm_hidden+global_dim)\n",
    "        # Process each vehicle:\n",
    "        vehicle_values = self.combine_fc(combined).squeeze(-1)  # (N, M) after unsqueeze\n",
    "        # Mask out invalid vehicles:\n",
    "        vehicle_values = vehicle_values * agent_mask.float()\n",
    "        return vehicle_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2,3,1)).squeeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(actor, critic, buffer, \n",
    "               actor_lr=3e-4, critic_lr=1e-3, \n",
    "               clip_ratio=0.2, train_iters=80):\n",
    "    data = buffer.get()\n",
    "    kn, lm, bd, mk, act, logp_old, ret, adv = data.values()\n",
    "\n",
    "    # optimizers\n",
    "    a_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    c_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    for _ in range(train_iters):\n",
    "        # Policy loss\n",
    "        dist, logp = actor.get_dist(kn, lm, bd, mk)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "        policy_loss = -torch.mean(torch.min(ratio * adv, clipped_ratio * adv))\n",
    "\n",
    "        # Value loss\n",
    "        value = critic(kn, lm, bd, mk)\n",
    "        value_loss = torch.mean((ret - value).pow(2))\n",
    "\n",
    "        # Update actor\n",
    "        a_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        a_optimizer.step()\n",
    "\n",
    "        # Update critic\n",
    "        c_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        c_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data, df = convert_highd_sample_to_gail_expert(\n",
    "    sample_csv=r\"./data/26_sample_tracks.csv\",\n",
    "    meta_csv=r\"E:\\Data\\highd-dataset-v1.0\\data\\26_recordingMeta.csv\",\n",
    "    forward=False,\n",
    "    p_agent=0.90\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your environment instance.\n",
    "env = HighwayEnv(generation_mode=True, demo_mode=False, T=50)\n",
    "# Optionally, set expert data:\n",
    "# expert_data, df = convert_highd_sample_to_gail_expert(...); env.set_expert_data(expert_data)\n",
    "\n",
    "# Create PPO actor (policy) and critic networks.\n",
    "# Let T = history length, M = max number of vehicles, F = features (e.g., 4).\n",
    "\n",
    "# Uncomment and update the following line when expert_data is available:\n",
    "env.set_expert_data(expert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HighwayEnvBuffer' object has no attribute 'path_start'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminal:\n\u001b[0;32m     21\u001b[0m         last_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m critic(\u001b[38;5;241m*\u001b[39mobs)\n\u001b[1;32m---> 22\u001b[0m         \u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m         obs, ep_ret, ep_len \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# after collecting data, perform PPO update\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[88], line 34\u001b[0m, in \u001b[0;36mHighwayEnvBuffer.finish_path\u001b[1;34m(self, last_val)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinish_path\u001b[39m(\u001b[38;5;28mself\u001b[39m, last_val):\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    last_val: (M,)  value estimates for step after the end\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     slice_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_start\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptr)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# append last_val to compute deltas\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     rews \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrew_buf[slice_], last_val\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (L+1, M)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HighwayEnvBuffer' object has no attribute 'path_start'"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = 300\n",
    "epochs = 50\n",
    "\n",
    "actor  = PPOActor(50, 100, 7)\n",
    "critic = PPOCritic(50, 100, 7)\n",
    "buf = HighwayEnvBuffer(50, 100, 7, 2, size=steps_per_epoch)\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    _obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    obs = _obs.values()\n",
    "    for t in range(steps_per_epoch):\n",
    "        action, logp = actor.get_action(*obs)\n",
    "        value = critic(*obs)\n",
    "        next_obs, rew, done, _ = env.step(action.squeeze(0).detach().numpy())\n",
    "        ep_ret += rew; ep_len += 1\n",
    "        buf.store(*obs, action.squeeze(0), logp.squeeze(0), rew, value.squeeze(0))\n",
    "\n",
    "        obs = next_obs.values()\n",
    "        terminal = done or (t == steps_per_epoch - 1)\n",
    "        if terminal:\n",
    "            last_val = 0 if done else critic(*obs)\n",
    "            buf.finish_path(last_val.squeeze(0))\n",
    "            obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # after collecting data, perform PPO update\n",
    "    ppo_update(actor, critic, buf)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} complete\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
