{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "from env import HighwayEnv, convert_highd_sample_to_gail_expert\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from buffer import HighwayEnvMemoryBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = HighwayEnvMemoryBuffer(300)\n",
    "\n",
    "expert_data, df = convert_highd_sample_to_gail_expert(\n",
    "    sample_csv=r\"./data/26_sample_tracks.csv\",\n",
    "    meta_csv=r\"E:\\Data\\highd-dataset-v1.0\\data\\26_recordingMeta.csv\",\n",
    "    forward=False,\n",
    "    p_agent=0.90\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and set expert data.\n",
    "env = HighwayEnv(dt=0.2, T=50, generation_mode=False, demo_mode=True)\n",
    "# Uncomment and update the following line when expert_data is available:\n",
    "env.set_expert_data(expert_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:04<00:00, 69.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "obs = env.reset() \n",
    "#\n",
    "for step in trange(NUM_STEPS):\n",
    "    # For demonstration, sample random actions for each vehicle slot.\n",
    "    # Action shape: (N_max, 2). Since have set DEMO_MODE=TRUE, the actions will be overwritten by the agents\n",
    "    action = torch.full((env.N_max, 2), 0.0)\n",
    "    log_prob = torch.full((env.N_max, 2), 1.0)\n",
    "    # Step the environment: we get new observation, reward, done, and info.\n",
    "    next_obs, reward, done, info = env.step(action)\n",
    "    buffer.push(obs, action, reward, log_prob, next_obs, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, action, reward, log_prob, next_obs, done = buffer.sample(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50, 100, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, T, M, F, lstm_hidden=64, global_dim=12, combined_hidden=64, output_size=2):\n",
    "        \"\"\"\n",
    "        Actor network for generating vehicle acceleration decisions.\n",
    "        \n",
    "        Parameters:\n",
    "          T (int): History length (number of time steps).\n",
    "          M (int): Maximum number of vehicles per sample.\n",
    "          F (int): Number of features per vehicle per timestep (e.g. [x, y, xVelocity, yVelocity]).\n",
    "          lstm_hidden (int): Hidden dimension for the LSTM.\n",
    "          global_dim (int): Dimension for processing the global inputs (lane markers + road boundaries).\n",
    "          combined_hidden (int): Hidden dimension in the combined FC layers.\n",
    "          output_size (int): Dimension of the acceleration output (typically 2).\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "        self.F = F\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        \n",
    "        # LSTM to process each vehicle's time-dependent sequence (shape: (T, F)).\n",
    "        self.lstm = nn.LSTM(input_size=F, hidden_size=lstm_hidden, batch_first=True)\n",
    "        \n",
    "        # Global information comes from lane markers (10) and road boundaries (2) => 12 values.\n",
    "        self.global_fc = nn.Linear(12, global_dim)\n",
    "        \n",
    "        # Combined fully connected layers mapping concatenated per-vehicle and global features to the output.\n",
    "        self.combine_fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + global_dim, combined_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_hidden, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, time_dep, lane_markers, boundaries):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "          time_dep (torch.Tensor): Time-dependent kinematics, shape (N, T, M, F).\n",
    "            (Missing entries are NaN; they are replaced by 0 before processing.)\n",
    "          lane_markers (torch.Tensor): Global lane markers, shape (N, 10). May contain NaNs.\n",
    "          boundaries (torch.Tensor): Global road boundaries, shape (N, 2). May contain NaNs.\n",
    "          \n",
    "        Returns:\n",
    "          accelerations (torch.Tensor): Output accelerations of shape (N, M, 2).\n",
    "        \"\"\"\n",
    "        N, T, M, F = time_dep.shape  # Unpack dimensions.\n",
    "        \n",
    "        # --- Process Time-Dependent Kinematics ---\n",
    "        # Rearrange to shape (N, M, T, F), then flatten the N and M dimensions: (N*M, T, F).\n",
    "        time_dep = time_dep.permute(0, 2, 1, 3).contiguous()\n",
    "        time_dep = time_dep.view(N * M, T, F)\n",
    "        \n",
    "        # Compute valid mask for each sequence (valid if not all features are NaN)\n",
    "        valid_mask = ~torch.all(torch.isnan(time_dep), dim=-1)  # shape: (N*M, T)\n",
    "        seq_lengths = valid_mask.sum(dim=1)  # shape: (N*M,)\n",
    "        # Ensure a minimum length of 1 for any sequence.\n",
    "        seq_lengths[seq_lengths == 0] = 1\n",
    "        \n",
    "        # Replace NaN values in the kinematics with 0.\n",
    "        time_dep_clean = torch.nan_to_num(time_dep, nan=0.0)\n",
    "        \n",
    "        # Pack the sequence so that the LSTM ignores padded time steps.\n",
    "        packed = rnn_utils.pack_padded_sequence(time_dep_clean, lengths=seq_lengths.cpu(),\n",
    "                                                  batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "        # For a single-layer LSTM, hn has shape (1, N*M, lstm_hidden)\n",
    "        hn = hn.squeeze(0)  # Now shape: (N*M, lstm_hidden)\n",
    "        \n",
    "        # Reshape to get per-vehicle representation: (N, M, lstm_hidden)\n",
    "        vehicle_repr = hn.view(N, M, self.lstm_hidden)\n",
    "        \n",
    "        # --- Process Global Information (lane markers and boundaries) ---\n",
    "        # Before processing, replace NaN values in lane markers and boundaries with 0.\n",
    "        # Alternatively, you might choose to use a meaningful default.\n",
    "        lane_markers_clean = torch.nan_to_num(lane_markers, nan=0.0)  # shape: (N, 10)\n",
    "        boundaries_clean = torch.nan_to_num(boundaries, nan=0.0)        # shape: (N, 2)\n",
    "        # Concatenate lane markers and boundaries into a single tensor: shape: (N, 12)\n",
    "        global_input = torch.cat([lane_markers_clean, boundaries_clean], dim=1)\n",
    "        global_info = torch.relu(self.global_fc(global_input))  # shape: (N, global_dim)\n",
    "        # Broadcast to each vehicle slot: shape becomes (N, M, global_dim)\n",
    "        global_info = global_info.unsqueeze(1).expand(-1, M, -1)\n",
    "        \n",
    "        # --- Combine and Generate Accelerations ---\n",
    "        # Concatenate per-vehicle representation with global info along the feature dimension.\n",
    "        combined = torch.cat([vehicle_repr, global_info], dim=-1)  # shape: (N, M, lstm_hidden + global_dim)\n",
    "        # Pass through FC layers to generate acceleration outputs.\n",
    "        accelerations = self.combine_fc(combined)  # shape: (N, M, output_size)\n",
    "        \n",
    "        # Note: even though accelerations has shape (N, M, 2), downstream you should use the agent mask to choose only valid outputs.\n",
    "        return accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork_(nn.Module):\n",
    "    def __init__(self, T, M, F, lstm_hidden=64, global_dim=12, combined_hidden=64, output_size=2):\n",
    "        \"\"\"\n",
    "        Actor network for generating vehicle acceleration decisions.\n",
    "        \n",
    "        Parameters:\n",
    "          T (int): History length (number of time steps).\n",
    "          M (int): Maximum number of vehicles per sample.\n",
    "          F (int): Number of features per vehicle per timestep (e.g. [x, y, xVelocity, yVelocity]).\n",
    "          lstm_hidden (int): Hidden dimension for the LSTM.\n",
    "          global_dim (int): Dimension for processing the global inputs (lane markers + road boundaries).\n",
    "          combined_hidden (int): Hidden dimension in the combined FC layers.\n",
    "          output_size (int): Dimension of the acceleration output (typically 2).\n",
    "        \"\"\"\n",
    "        super(ActorNetwork_, self).__init__()\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "        self.F = F\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        \n",
    "        # LSTM to process each vehicle's time-dependent sequence (shape: (T, F)).\n",
    "        self.lstm = nn.LSTM(input_size=F, hidden_size=lstm_hidden, batch_first=True)\n",
    "        \n",
    "        # Global information comes from lane markers (10) and road boundaries (2) => 12 values.\n",
    "        # We'll process the lane markers after masking them.\n",
    "        self.global_fc = nn.Linear(10, global_dim)  # process lane markers separately\n",
    "        self.boundary_fc = nn.Linear(2, global_dim)   # process boundaries separately\n",
    "        \n",
    "        # Combine global features (concatenated) and map to final global representation.\n",
    "        self.global_combine_fc = nn.Linear(2 * global_dim, global_dim)\n",
    "        \n",
    "        # Combined fully connected layers mapping concatenated per-vehicle and global features to the output.\n",
    "        self.combine_fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + global_dim, combined_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_hidden, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, time_dep, lane_markers, boundaries):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "          time_dep (torch.Tensor): Time-dependent kinematics, shape (N, T, M, F).\n",
    "            (Missing entries are NaN; they are replaced by 0 before processing.)\n",
    "          lane_markers (torch.Tensor): Global lane markers, shape (N, 10). May contain NaNs.\n",
    "          boundaries (torch.Tensor): Global road boundaries, shape (N, 2). (Assumed to be complete.)\n",
    "          \n",
    "        Returns:\n",
    "          accelerations (torch.Tensor): Output accelerations of shape (N, M, 2).\n",
    "        \"\"\"\n",
    "        N, T, M, F = time_dep.shape  # Unpack dimensions\n",
    "        \n",
    "        # --- Process Time-Dependent Kinematics ---\n",
    "        # Rearrange to shape (N, M, T, F) and flatten to (N*M, T, F)\n",
    "        time_dep = time_dep.permute(0, 2, 1, 3).contiguous().view(N * M, T, F)\n",
    "        \n",
    "        # Compute valid mask: a time step is valid if not all F values are NaN.\n",
    "        valid_mask = ~torch.all(torch.isnan(time_dep), dim=-1)  # (N*M, T)\n",
    "        seq_lengths = valid_mask.sum(dim=1)  # (N*M,)\n",
    "        seq_lengths[seq_lengths == 0] = 1\n",
    "        \n",
    "        # Replace NaNs with 0 in the time-dependent input.\n",
    "        time_dep_clean = torch.nan_to_num(time_dep, nan=0.0)\n",
    "        \n",
    "        # Pack padded sequence and process with LSTM.\n",
    "        packed = rnn_utils.pack_padded_sequence(time_dep_clean, lengths=seq_lengths.cpu(),\n",
    "                                                  batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "        # hn: (num_layers, N*M, lstm_hidden) → assume single layer and squeeze:\n",
    "        hn = hn.squeeze(0)  # (N*M, lstm_hidden)\n",
    "        \n",
    "        # Reshape vehicle representation to (N, M, lstm_hidden)\n",
    "        vehicle_repr = hn.view(N, M, self.lstm_hidden)\n",
    "        \n",
    "        # --- Process Global Information with Masking for Lane Markers ---\n",
    "        # lane_markers: (N, 10)\n",
    "        # Create a mask for valid lane markers: 1 where not NaN, 0 where NaN.\n",
    "        lane_mask = (~torch.isnan(lane_markers)).float()  # (N, 10)\n",
    "        # Replace NaNs with 0.\n",
    "        lane_markers_clean = torch.nan_to_num(lane_markers, nan=0.0)\n",
    "        # Process lane markers via a linear layer.\n",
    "        global_lane_features = torch.relu(self.global_fc(lane_markers_clean))  # (N, global_dim)\n",
    "        # Scale lane features by the fraction of valid markers.\n",
    "        valid_ratio = lane_mask.mean(dim=1, keepdim=True)  # (N, 1)\n",
    "        global_lane_features = global_lane_features * valid_ratio\n",
    "        \n",
    "        # Process boundaries via another linear layer.\n",
    "        global_boundaries_features = torch.relu(self.boundary_fc(boundaries))  # (N, global_dim)\n",
    "        \n",
    "        # Combine the two global features.\n",
    "        global_combined = torch.cat([global_lane_features, global_boundaries_features], dim=1)  # (N, 2*global_dim)\n",
    "        global_info = torch.relu(self.global_combine_fc(global_combined))  # (N, global_dim)\n",
    "        # Broadcast global_info to each vehicle slot: (N, M, global_dim)\n",
    "        global_info = global_info.unsqueeze(1).expand(-1, M, -1)\n",
    "        \n",
    "        # --- Combine Per-Vehicle and Global Representations ---\n",
    "        combined = torch.cat([vehicle_repr, global_info], dim=-1)  # (N, M, lstm_hidden+global_dim)\n",
    "        accelerations = self.combine_fc(combined)  # (N, M, output_size) (typically 2: [xAccel, yAccel])\n",
    "        \n",
    "        return accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorNetwork_(50, 100, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dep, lane_markers, boundaries = obs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/1000 [00:00<00:17, 55.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/1000, Loss: 0.0218\n",
      "Iteration 20/1000, Loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 37/1000 [00:00<00:15, 61.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30/1000, Loss: 0.0033\n",
      "Iteration 40/1000, Loss: 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 58/1000 [00:00<00:15, 61.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50/1000, Loss: 0.0019\n",
      "Iteration 60/1000, Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 79/1000 [00:01<00:14, 62.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70/1000, Loss: 0.0022\n",
      "Iteration 80/1000, Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:01<00:14, 63.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90/1000, Loss: 0.0013\n",
      "Iteration 100/1000, Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 121/1000 [00:01<00:13, 64.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110/1000, Loss: 0.0001\n",
      "Iteration 120/1000, Loss: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 135/1000 [00:02<00:13, 62.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130/1000, Loss: 0.0014\n",
      "Iteration 140/1000, Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 156/1000 [00:02<00:14, 59.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150/1000, Loss: 0.0008\n",
      "Iteration 160/1000, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 180/1000 [00:02<00:13, 59.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 170/1000, Loss: 0.0007\n",
      "Iteration 180/1000, Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/1000 [00:03<00:13, 57.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190/1000, Loss: 0.0012\n",
      "Iteration 200/1000, Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 218/1000 [00:03<00:13, 57.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 210/1000, Loss: 0.0016\n",
      "Iteration 220/1000, Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 236/1000 [00:03<00:14, 54.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 230/1000, Loss: 0.0018\n",
      "Iteration 240/1000, Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 256/1000 [00:04<00:12, 58.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 250/1000, Loss: 0.0002\n",
      "Iteration 260/1000, Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 276/1000 [00:04<00:12, 59.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 270/1000, Loss: 0.0014\n",
      "Iteration 280/1000, Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 296/1000 [00:04<00:11, 59.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 290/1000, Loss: 0.0011\n",
      "Iteration 300/1000, Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 316/1000 [00:05<00:11, 59.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 310/1000, Loss: 0.0013\n",
      "Iteration 320/1000, Loss: 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 337/1000 [00:05<00:10, 61.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 330/1000, Loss: 0.0003\n",
      "Iteration 340/1000, Loss: 0.0015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 358/1000 [00:05<00:10, 61.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 350/1000, Loss: 0.0007\n",
      "Iteration 360/1000, Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 379/1000 [00:06<00:09, 62.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 370/1000, Loss: 0.0009\n",
      "Iteration 380/1000, Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [00:06<00:09, 62.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 390/1000, Loss: 0.0014\n",
      "Iteration 400/1000, Loss: 0.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 421/1000 [00:06<00:09, 62.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 410/1000, Loss: 0.0016\n",
      "Iteration 420/1000, Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 442/1000 [00:07<00:08, 62.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 430/1000, Loss: 0.0011\n",
      "Iteration 440/1000, Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 456/1000 [00:07<00:08, 62.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 450/1000, Loss: 0.0014\n",
      "Iteration 460/1000, Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 477/1000 [00:07<00:08, 63.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 470/1000, Loss: 0.0013\n",
      "Iteration 480/1000, Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 498/1000 [00:08<00:08, 61.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 490/1000, Loss: 0.0010\n",
      "Iteration 500/1000, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 519/1000 [00:08<00:07, 61.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 510/1000, Loss: 0.0008\n",
      "Iteration 520/1000, Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 540/1000 [00:08<00:07, 63.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 530/1000, Loss: 0.0013\n",
      "Iteration 540/1000, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 561/1000 [00:09<00:06, 63.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 550/1000, Loss: 0.0009\n",
      "Iteration 560/1000, Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 575/1000 [00:09<00:06, 63.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 570/1000, Loss: 0.0015\n",
      "Iteration 580/1000, Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 596/1000 [00:09<00:06, 63.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 590/1000, Loss: 0.0007\n",
      "Iteration 600/1000, Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 617/1000 [00:10<00:05, 64.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 610/1000, Loss: 0.0018\n",
      "Iteration 620/1000, Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 638/1000 [00:10<00:05, 64.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 630/1000, Loss: 0.0012\n",
      "Iteration 640/1000, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 659/1000 [00:10<00:05, 62.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 650/1000, Loss: 0.0009\n",
      "Iteration 660/1000, Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 680/1000 [00:11<00:05, 63.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 670/1000, Loss: 0.0001\n",
      "Iteration 680/1000, Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [00:11<00:04, 62.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 690/1000, Loss: 0.0012\n",
      "Iteration 700/1000, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 722/1000 [00:11<00:04, 62.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 710/1000, Loss: 0.0019\n",
      "Iteration 720/1000, Loss: 0.0023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 736/1000 [00:11<00:04, 61.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 730/1000, Loss: 0.0004\n",
      "Iteration 740/1000, Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 757/1000 [00:12<00:03, 62.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 750/1000, Loss: 0.0009\n",
      "Iteration 760/1000, Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 778/1000 [00:12<00:03, 62.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 770/1000, Loss: 0.0012\n",
      "Iteration 780/1000, Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 799/1000 [00:13<00:03, 60.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 790/1000, Loss: 0.0015\n",
      "Iteration 800/1000, Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 820/1000 [00:13<00:02, 60.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 810/1000, Loss: 0.0012\n",
      "Iteration 820/1000, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 841/1000 [00:13<00:02, 60.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 830/1000, Loss: 0.0008\n",
      "Iteration 840/1000, Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 862/1000 [00:14<00:02, 61.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 850/1000, Loss: 0.0004\n",
      "Iteration 860/1000, Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 876/1000 [00:14<00:02, 61.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 870/1000, Loss: 0.0008\n",
      "Iteration 880/1000, Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 897/1000 [00:14<00:01, 61.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 890/1000, Loss: 0.0012\n",
      "Iteration 900/1000, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 918/1000 [00:14<00:01, 62.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 910/1000, Loss: 0.0009\n",
      "Iteration 920/1000, Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 939/1000 [00:15<00:00, 62.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 930/1000, Loss: 0.0013\n",
      "Iteration 940/1000, Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 960/1000 [00:15<00:00, 61.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 950/1000, Loss: 0.0017\n",
      "Iteration 960/1000, Loss: 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 981/1000 [00:15<00:00, 62.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 970/1000, Loss: 0.0005\n",
      "Iteration 980/1000, Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:16<00:00, 61.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 990/1000, Loss: 0.0006\n",
      "Iteration 1000/1000, Loss: 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "actor.train()  # set to training mode\n",
    "\n",
    "# Define an optimizer.\n",
    "optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop: We'll do a few iterations updating the network using a fake loss defined as the mean absolute acceleration.\n",
    "num_iterations = 1000\n",
    "for i in trange(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass.\n",
    "    accel_output = actor(time_dep, lane_markers, boundaries)  # shape (N, M, 2)\n",
    "    # Compute a fake loss: mean of the absolute acceleration values.\n",
    "    loss = torch.abs(accel_output).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Iteration {i+1}/{num_iterations}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         ...,\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210]],\n",
       "\n",
       "        [[-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         ...,\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210]],\n",
       "\n",
       "        [[-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         ...,\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         ...,\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210]],\n",
       "\n",
       "        [[-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         ...,\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210]],\n",
       "\n",
       "        [[-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         ...,\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210],\n",
       "         [-0.0430, -0.0210]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor(obs[0], obs[1], obs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = random.sample(buffer.buffer, 64)\n",
    "states, actions, rewards, next_states, dones = zip(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_keys=['time_dependent','time_independent', 'lane_markers', 'boundary_lines', 'agent_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ torch.stack([obs[key] for obs in states]) for key in state_keys ][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import numpy as np\n",
    "\n",
    "#########################################\n",
    "# Utility function for packing sequences\n",
    "#########################################\n",
    "def compute_seq_lengths(time_dep):\n",
    "    \"\"\"\n",
    "    Given time_dep of shape (B, T, F), return a tensor of sequence lengths,\n",
    "    where a timestep is considered valid if not all features are NaN.\n",
    "    \"\"\"\n",
    "    # valid if not all F values are NaN:\n",
    "    valid_mask = ~torch.all(torch.isnan(time_dep), dim=-1)  # shape (B, T)\n",
    "    seq_lengths = valid_mask.sum(dim=1)  # (B,)\n",
    "    seq_lengths[seq_lengths == 0] = 1  # avoid zeros\n",
    "    return seq_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Actor Network (Policy)\n",
    "#########################################\n",
    "class PPOActor(nn.Module):\n",
    "    def __init__(self, T, M, F, lstm_hidden=64, global_dim=12, combined_hidden=64, output_size=2):\n",
    "        \"\"\"\n",
    "        Actor network that takes the structured observation and outputs\n",
    "        a Gaussian distribution over acceleration commands.\n",
    "        \n",
    "        Observation:\n",
    "           - time_dep: (N, T, M, F) \n",
    "           - lane_markers: (N, 10)\n",
    "           - boundary_lines: (N, 2)\n",
    "        Output:\n",
    "           - accelerations: (N, M, 2)\n",
    "        \"\"\"\n",
    "        super(PPOActor, self).__init__()\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "        self.F = F\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        \n",
    "        # LSTM for time-dependent kinematics (for each vehicle's sequence of length T and F features).\n",
    "        self.lstm = nn.LSTM(input_size=F, hidden_size=lstm_hidden, batch_first=True)\n",
    "        \n",
    "        # Global information: process lane markers and boundaries separately.\n",
    "        self.global_fc = nn.Linear(10, global_dim)  # lane markers of shape (N, 10)\n",
    "        self.boundary_fc = nn.Linear(2, global_dim)   # boundaries of shape (N, 2)\n",
    "        self.global_combine_fc = nn.Linear(2 * global_dim, global_dim)\n",
    "        \n",
    "        # Fully connected layers that combine per-vehicle features with global info.\n",
    "        self.combine_fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + global_dim, combined_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "        # Learnable log standard deviation for the Gaussian distribution over actions.\n",
    "        # We use a parameter of shape (1, 1, output_size) that will be broadcast.\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, 1, output_size))\n",
    "        \n",
    "    def forward(self, time_dep, lane_markers, boundaries):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "           time_dep: (N, T, M, F)\n",
    "           lane_markers: (N, 10)  (may contain NaNs)\n",
    "           boundaries: (N, 2)\n",
    "        \n",
    "        Returns:\n",
    "           mean: (N, M, 2) — the mean acceleration for each vehicle slot.\n",
    "           log_std: (N, M, 2) — the log_std, broadcasted along N and M.\n",
    "        \"\"\"\n",
    "        if time_dep.dim() == 3:\n",
    "            time_dep = time_dep.unsqueeze(0) \n",
    "        if lane_markers.dim() == 1:\n",
    "            lane_markers = lane_markers.unsqueeze(0) \n",
    "        if boundaries.dim() == 1:\n",
    "            boundaries = boundaries.unsqueeze(0) \n",
    "\n",
    "        N, T, M, _F = time_dep.shape\n",
    "        \n",
    "        # Process time-dependent kinematics:\n",
    "        # Permute to (N, M, T, F) then flatten N and M to get (N*M, T, F)\n",
    "        time_dep = time_dep.permute(0, 2, 1, 3).contiguous().view(N * M, T, _F)\n",
    "        # Replace NaNs with 0:\n",
    "        time_dep_clean = torch.nan_to_num(time_dep, nan=0.0)\n",
    "        # Compute sequence lengths per vehicle:\n",
    "        seq_lengths = compute_seq_lengths(time_dep)\n",
    "        assert not torch.isnan(time_dep_clean).any()\n",
    "        # Pack padded sequence:\n",
    "        packed = rnn_utils.pack_padded_sequence(time_dep_clean, lengths=seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "        # hn: (1, N*M, lstm_hidden) -> squeeze to (N*M, lstm_hidden)\n",
    "        hn = hn.squeeze(0)\n",
    "        # Reshape back to (N, M, lstm_hidden)\n",
    "        vehicle_repr = hn.view(N, M, self.lstm_hidden)\n",
    "        \n",
    "        # Process global inputs:\n",
    "        # For lane markers, replace NaNs with 0 and create a mask.\n",
    "        lane_markers_clean = torch.nan_to_num(lane_markers, nan=0.0)  # (N, 10)\n",
    "        lane_mask = (~torch.isnan(lane_markers)).float()  # (N, 10)\n",
    "        global_lane_features = F.relu(self.global_fc(lane_markers_clean))  # (N, global_dim)\n",
    "        valid_ratio = lane_mask.mean(dim=1, keepdim=True)  # (N, 1)\n",
    "        global_lane_features = global_lane_features * valid_ratio\n",
    "        \n",
    "        global_boundaries_features = F.relu(self.boundary_fc(boundaries))  # (N, global_dim)\n",
    "        global_combined = torch.cat([global_lane_features, global_boundaries_features], dim=1)  # (N, 2*global_dim)\n",
    "        global_info = F.relu(self.global_combine_fc(global_combined))  # (N, global_dim)\n",
    "        global_info = global_info.unsqueeze(1).expand(-1, M, -1)  # (N, M, global_dim)\n",
    "        \n",
    "        # Combine vehicle representation with global info:\n",
    "        combined = torch.cat([vehicle_repr, global_info], dim=-1)  # (N, M, lstm_hidden + global_dim)\n",
    "        mean = self.combine_fc(combined)  # (N, M, output_size)\n",
    "        \n",
    "        # Broadcast log_std:\n",
    "        log_std = self.log_std.expand(N, M, -1)  # (N, M, output_size)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def get_action(self, time_dep, lane_markers, boundaries, agent_mask):\n",
    "        \"\"\"\n",
    "        Sample actions from the policy distribution.\n",
    "        \n",
    "        Returns:\n",
    "           action: (N, M, output_size)\n",
    "           log_prob: (N, M, output_size) or summed over output_size per vehicle.\n",
    "        \"\"\"\n",
    "        _agent_mask = agent_mask.unsqueeze(0) if agent_mask.dim() == 1 else agent_mask\n",
    "        mean, log_std = self.forward(time_dep, lane_markers, boundaries)\n",
    "        # print (mean, log_std)\n",
    "        std = torch.exp(log_std)\n",
    "        # Create a normal distribution per vehicle slot.\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        # Sample actions using reparameterization (this allows for differentiable sampling).\n",
    "        action = dist.rsample()  # shape (N, M, output_size)\n",
    "        # Compute log probabilities.\n",
    "        log_prob = dist.log_prob(action)  # shape (N, M, output_size)\n",
    "        log_prob = log_prob.sum(dim=-1)  # aggregate over action dimensions, shape (N, M)\n",
    "        # Mask out non-agent slots: set log_prob to 0 for non-agent vehicles.\n",
    "        # This means that when computing the loss, only entries with agent_mask==1 will contribute.\n",
    "        masked_log_prob = log_prob * _agent_mask  # agent_mask is assumed to be float, with 1 or 0.\n",
    "        masked_action = action * _agent_mask.unsqueeze(-1) \n",
    "        \n",
    "        return masked_action, masked_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Critic Network (Value Function)\n",
    "#########################################\n",
    "class PPOCritic(nn.Module):\n",
    "    def __init__(self, T, M, F, lstm_hidden=64, global_dim=12, combined_hidden=64):\n",
    "        \"\"\"\n",
    "        Critic network that takes the same observation and outputs a scalar value for each sample.\n",
    "        \n",
    "        In many cases we want a state value per sample (N,1) that is derived from the entire observation.\n",
    "        We can process each vehicle's time-dependent sequence similar to the actor,\n",
    "        combine with global features, and then aggregate (for instance, by averaging over vehicles).\n",
    "        \"\"\"\n",
    "        super(PPOCritic, self).__init__()\n",
    "        self.T = T\n",
    "        self.M = M\n",
    "        self.F = F\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        \n",
    "        # LSTM for time-dependent kinematics (per vehicle):\n",
    "        self.lstm = nn.LSTM(input_size=F, hidden_size=lstm_hidden, batch_first=True)\n",
    "        \n",
    "        # Global information processing:\n",
    "        self.global_fc = nn.Linear(10, global_dim)  # for lane markers\n",
    "        self.boundary_fc = nn.Linear(2, global_dim)   # for boundaries\n",
    "        self.global_combine_fc = nn.Linear(2 * global_dim, global_dim)\n",
    "        \n",
    "        # Combine per-vehicle features with global info and aggregate:\n",
    "        # We combine each vehicle's representation and then average over vehicles.\n",
    "        self.combine_fc = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden + global_dim, combined_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(combined_hidden, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, time_dep, lane_markers, boundaries, agent_mask):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "           time_dep: (N, T, M, F)\n",
    "           lane_markers: (N, 10)\n",
    "           boundaries: (N, 2)\n",
    "           agent_mask: (N, M) binary mask for valid vehicles.\n",
    "           \n",
    "        Returns:\n",
    "           values: (N, 1) scalar state-value estimates.\n",
    "        \"\"\"\n",
    "        if time_dep.dim() == 3:\n",
    "            time_dep = time_dep.unsqueeze(0) \n",
    "        if lane_markers.dim() == 1:\n",
    "            lane_markers = lane_markers.unsqueeze(0) \n",
    "        if boundaries.dim() == 1:\n",
    "            boundaries = boundaries.unsqueeze(0) \n",
    "        N, T, M, _F = time_dep.shape\n",
    "        \n",
    "        # Process time-dependent input as before:\n",
    "        time_dep = time_dep.permute(0, 2, 1, 3).contiguous().view(N * M, T, _F)\n",
    "        time_dep_clean = torch.nan_to_num(time_dep, nan=0.0)\n",
    "        valid_mask = ~torch.all(torch.isnan(time_dep), dim=-1)  # (N*M, T)\n",
    "        seq_lengths = valid_mask.sum(dim=1)\n",
    "        seq_lengths[seq_lengths == 0] = 1\n",
    "        packed = rnn_utils.pack_padded_sequence(time_dep_clean, lengths=seq_lengths.cpu(),\n",
    "                                                  batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "        hn = hn.squeeze(0)  # (N*M, lstm_hidden)\n",
    "        vehicle_repr = hn.view(N, M, self.lstm_hidden)\n",
    "        \n",
    "        # Process global inputs:\n",
    "        lane_markers_clean = torch.nan_to_num(lane_markers, nan=0.0)\n",
    "        lane_mask = (~torch.isnan(lane_markers)).float()\n",
    "        global_lane_features = F.relu(self.global_fc(lane_markers_clean))  # (N, global_dim)\n",
    "        valid_ratio = lane_mask.mean(dim=1, keepdim=True)\n",
    "        global_lane_features = global_lane_features * valid_ratio\n",
    "        \n",
    "        global_boundaries_features = F.relu(self.boundary_fc(boundaries))  # (N, global_dim)\n",
    "        global_combined = torch.cat([global_lane_features, global_boundaries_features], dim=1)\n",
    "        global_info = F.relu(self.global_combine_fc(global_combined))  # (N, global_dim)\n",
    "        global_info = global_info.unsqueeze(1).expand(-1, M, -1)  # (N, M, global_dim)\n",
    "        \n",
    "        # Combine vehicle representation and global features.\n",
    "        combined = torch.cat([vehicle_repr, global_info], dim=-1)  # (N, M, lstm_hidden+global_dim)\n",
    "        # Process each vehicle:\n",
    "        vehicle_values = self.combine_fc(combined)  # (N, M, 1)\n",
    "        # Mask out invalid vehicles:\n",
    "        mask = agent_mask.unsqueeze(-1).float()  # (N, M, 1)\n",
    "        vehicle_values = vehicle_values * mask\n",
    "        # Aggregate values from each vehicle (e.g., average only over valid vehicles):\n",
    "        sum_values = vehicle_values.sum(dim=1)  # (N, 1)\n",
    "        count = mask.sum(dim=1)  # (N, 1)\n",
    "        # To avoid division by zero:\n",
    "        count = torch.clamp(count, min=1.0)\n",
    "        state_values = sum_values / count  # (N, 1)\n",
    "        return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# PPO Update and Training Loop (Outline)\n",
    "#########################################\n",
    "def ppo_update(policy_net, critic_net, optimizer_policy, optimizer_critic,\n",
    "               observations, actions, log_probs_old, returns, advantages,\n",
    "               clip_epsilon=0.2, value_coef=0.5, entropy_coef=0.01):\n",
    "    \"\"\"\n",
    "    Perform one update step of PPO given a batch of data.\n",
    "    \n",
    "    observations: a dict containing keys: 'time_dependent', 'lane_markers', 'boundary_lines', 'mask'\n",
    "    actions: tensor of shape (N, M, 2)\n",
    "    log_probs_old: tensor of shape (N, M), the log probability of actions under the old policy.\n",
    "    returns: tensor of shape (N,), estimated returns.\n",
    "    advantages: tensor of shape (N,), estimated advantages.\n",
    "    \n",
    "    Note: In this simple outline, we assume that the policy is applied per sample (with N samples).\n",
    "    \"\"\"\n",
    "    # Unpack observations:\n",
    "    time_dep, lane_markers, boundaries, mask = observations.values()   \n",
    "\n",
    "    # Forward pass through the actor network:\n",
    "    mean, log_std = policy_net.forward(time_dep, lane_markers, boundaries)\n",
    "    std = torch.exp(log_std)\n",
    "    \n",
    "    # Create the current policy distribution:\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    log_probs = dist.log_prob(actions)  # (N, M, 2)\n",
    "    log_probs = log_probs.sum(dim=-1)    # Sum over action dimensions to get (N, M)\n",
    "    \n",
    "    # For simplicity, average log_probs over valid vehicles per sample.\n",
    "    mask_float = mask.float().unsqueeze(-1)\n",
    "    # print (mask.shape, log_probs.shape, mask_float.shape)\n",
    "    log_probs_sample = (log_probs * mask_float).sum(dim=1) / torch.clamp(mask_float.sum(dim=1), min=1.0)\n",
    "    log_probs_old_sample = (log_probs_old * mask_float).sum(dim=1) / torch.clamp(mask_float.sum(dim=1), min=1.0)\n",
    "    \n",
    "    # Compute probability ratio:\n",
    "    ratio = torch.exp(log_probs_sample - log_probs_old_sample)  # (N,)\n",
    "\n",
    "    # Compute PPO actor loss with clipping:\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "    actor_loss = -torch.mean(torch.min(surr1, surr2))\n",
    "    \n",
    "    # Compute critic loss (MSE between value estimate and returns):\n",
    "    values = critic_net.forward(time_dep, lane_markers, boundaries, mask)  # (N, 1)\n",
    "    values = values.squeeze(-1)\n",
    "    critic_loss = F.mse_loss(values, returns)\n",
    "    \n",
    "    # Compute entropy bonus:\n",
    "    entropy = dist.entropy().sum(dim=-1)  # (N, M) → sum over action dimensions.\n",
    "    entropy = (entropy * mask_float).sum(dim=1) / torch.clamp(mask_float.sum(dim=1), min=1.0)\n",
    "    entropy_bonus = torch.mean(entropy)\n",
    "    \n",
    "    loss = actor_loss + value_coef * critic_loss - entropy_coef * entropy_bonus\n",
    "    \n",
    "    optimizer_policy.zero_grad()\n",
    "    optimizer_critic.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_policy.step()\n",
    "    optimizer_critic.step()\n",
    "    \n",
    "    return loss.item(), actor_loss.item(), critic_loss.item(), entropy_bonus.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For completeness, here is a simple discounting function.\n",
    "def compute_returns(rewards, gamma):\n",
    "    \"\"\"\n",
    "    Compute discounted returns from a list of rewards.\n",
    "    Args:\n",
    "        rewards (list[float]): List of rewards over a rollout (length L).\n",
    "        gamma (float): Discount factor.\n",
    "    Returns:\n",
    "        torch.Tensor: Returns as a tensor of shape (L,)\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    R = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return torch.tensor(returns, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training:\n",
    "num_epochs = 1000          # number of PPO iterations\n",
    "rollout_steps = 300        # number of steps in each rollout (or use env.total_steps if defined)\n",
    "gamma = 0.99               # discount factor\n",
    "clip_epsilon = 0.2         # PPO clipping epsilon\n",
    "value_coef = 0.5           # coefficient for value loss\n",
    "entropy_coef = 0.01        # coefficient for entropy bonus\n",
    "\n",
    "# Create your environment instance.\n",
    "env = HighwayEnv(generation_mode=True, demo_mode=False, T=50)\n",
    "# Optionally, set expert data:\n",
    "# expert_data, df = convert_highd_sample_to_gail_expert(...); env.set_expert_data(expert_data)\n",
    "\n",
    "# Create PPO actor (policy) and critic networks.\n",
    "# Let T = history length, M = max number of vehicles, F = features (e.g., 4).\n",
    "\n",
    "# Uncomment and update the following line when expert_data is available:\n",
    "env.set_expert_data(expert_data)\n",
    "\n",
    "T = env.T\n",
    "M = env.N_max\n",
    "_F = 7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6012, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_62220\\2139169469.py:47: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(values, returns)\n",
      "  0%|          | 1/1000 [00:06<1:45:52,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3147, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:11<1:35:57,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1036, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:17<1:36:42,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4542, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:23<1:36:36,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5779, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [00:29<1:36:39,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1665, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [00:35<1:37:57,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6875, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:41<1:37:13,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4623, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [00:47<1:39:09,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4445, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [00:53<1:40:23,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1270, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [00:59<1:38:32,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: Total Reward = 0.00, Loss = 0.0622, Actor Loss = -1.9143, Critic Loss = 4.5241, Entropy = 28.5562\n",
      "tensor(1.4430, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [01:05<1:37:44,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5593, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [01:11<1:46:57,  6.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m (advantage_final)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Call PPO update function with the collected final transition data.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# (A full implementation would sample multiple mini-batches over all rollout data.)\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m loss, actor_loss, critic_loss, entropy_bonus \u001b[38;5;241m=\u001b[39m \u001b[43mppo_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43moptimizer_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_critic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mobservations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mlog_probs_old\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_log_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mreturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturns_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43madvantages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madvantage_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mclip_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mvalue_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mentropy_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentropy_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Total Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Actor Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactor_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Critic Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcritic_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Entropy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentropy_bonus\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[84], line 58\u001b[0m, in \u001b[0;36mppo_update\u001b[1;34m(policy_net, critic_net, optimizer_policy, optimizer_critic, observations, actions, log_probs_old, returns, advantages, clip_epsilon, value_coef, entropy_coef)\u001b[0m\n\u001b[0;32m     56\u001b[0m optimizer_policy\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     57\u001b[0m optimizer_critic\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 58\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m optimizer_policy\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     60\u001b[0m optimizer_critic\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Productivity\\Anaconda\\envs\\deep\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Productivity\\Anaconda\\envs\\deep\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "actor = PPOActor(T=T, M=M, F=_F, lstm_hidden=64, global_dim=12, combined_hidden=64, output_size=2)\n",
    "critic = PPOCritic(T=T, M=M, F=_F, lstm_hidden=64, global_dim=12, combined_hidden=64)\n",
    "\n",
    "actor.train()\n",
    "critic.train()\n",
    "\n",
    "# Create optimizers.\n",
    "optimizer_policy = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=1e-3)\n",
    "# PPO training loop:\n",
    "for epoch in trange(num_epochs):\n",
    "    # Rollout: collect one trajectory from the environment.\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rollout_data = []  # list to store transitions: (obs, action, reward, log_prob)\n",
    "    total_reward = 0.0\n",
    "    step = 0\n",
    "    # Roll out for a fixed number of steps or until done.\n",
    "    while not done and step < rollout_steps:\n",
    "        # Get action and log probability from the actor network.\n",
    "        with torch.no_grad():\n",
    "            # actor.get_action returns: action (N, M, 2) and log_prob (N, M)\n",
    "            action, log_prob = actor.get_action(obs['time_dependent'], obs['lane_markers'], obs['boundary_lines'], obs['agent_mask'])\n",
    "        next_obs, reward, done, info = env.step(action[0])\n",
    "        # print (next_obs)\n",
    "        rollout_data.append((obs, action[0], reward, log_prob[0]))\n",
    "        total_reward += reward  # reward is scalar (from environment) per step\n",
    "        obs = next_obs\n",
    "        step += 1\n",
    "\n",
    "    # Now, assume that the rollout_data is a list of length L (the number of steps collected).\n",
    "    # We need to compute returns and advantages per rollout step.\n",
    "    # For simplicity, we treat the scalar reward per step as common across the batch dimension (N).\n",
    "    rewards = [transition[2] for transition in rollout_data]  # list of rewards, length L\n",
    "    returns = compute_returns(rewards, gamma)  # tensor shape: (L,)\n",
    "    # Compute critic values for each step (we use the \"time_dependent\" observation and associated global inputs)\n",
    "    critic_values = []\n",
    "    for (obs, _, _, _) in rollout_data:\n",
    "        # Critic forward pass: returns shape (N, 1)\n",
    "        value = critic.forward(obs['time_dependent'], obs['lane_markers'], obs['boundary_lines'], obs['agent_mask'])\n",
    "        # For simplicity, we average over the vehicle (M) dimension and then take the mean over the batch (N).\n",
    "        # (In practice, you might compute a more refined advantage estimate.)\n",
    "        critic_values.append(value.mean())\n",
    "    critic_values = torch.stack(critic_values)  # shape (L,)\n",
    "    advantages = returns - critic_values  # shape (L,)\n",
    "    # For the PPO update, we need to build a mini-batch. In this simplified version, we average across the rollout\n",
    "    # and use the final observation's data as representative.\n",
    "    final_obs, final_action, final_log_prob = rollout_data[-1][0], rollout_data[-1][1], rollout_data[-1][3] \n",
    "    returns_final = returns[-1]   # scalar\n",
    "    advantage_final = advantages[-1]  # scalar\n",
    "    # Call PPO update function with the collected final transition data.\n",
    "    # (A full implementation would sample multiple mini-batches over all rollout data.)\n",
    "    loss, actor_loss, critic_loss, entropy_bonus = ppo_update(actor, critic,\n",
    "                                                              optimizer_policy, optimizer_critic,\n",
    "                                                              observations=final_obs,\n",
    "                                                              actions=final_action,\n",
    "                                                              log_probs_old=final_log_prob,\n",
    "                                                              returns=returns_final,\n",
    "                                                              advantages=advantage_final,\n",
    "                                                              clip_epsilon=clip_epsilon,\n",
    "                                                              value_coef=value_coef,\n",
    "                                                              entropy_coef=entropy_coef)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Total Reward = {total_reward:.2f}, Loss = {loss:.4f}, Actor Loss = {actor_loss:.4f}, Critic Loss = {critic_loss:.4f}, Entropy = {entropy_bonus:.4f}\")\n",
    "\n",
    "print(\"PPO training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1,100)).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_dependent': tensor([[[338.5528,  12.2227, -16.0645,  ...,   4.6378,   1.8017,  12.9250],\n",
       "          [328.8494,  11.1394, -16.0059,  ...,   5.1959,   1.7971,  12.9250],\n",
       "          [246.1113,  12.1972, -14.1193,  ...,   5.0208,   1.8035,  12.9250],\n",
       "          ...,\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
       " \n",
       "         [[335.3517,  12.0744, -15.9469,  ...,   4.6378,   1.8017,  12.9250],\n",
       "          [325.6366,  10.9188, -16.1223,  ...,   5.1959,   1.7971,  12.9250],\n",
       "          [243.2651,  12.4896, -14.3428,  ...,   5.0208,   1.8035,  12.9250],\n",
       "          ...,\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
       " \n",
       "         [[332.1639,  11.9514, -15.9309,  ...,   4.6378,   1.8017,  12.9250],\n",
       "          [322.4199,  10.7331, -16.0445,  ...,   5.1959,   1.7971,  12.9250],\n",
       "          [240.4255,  12.8081, -14.0538,  ...,   5.0208,   1.8035,  12.9250],\n",
       "          ...,\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[366.2784,  13.4796, -16.0794,  ...,   4.7065,   1.8098,  12.9250],\n",
       "          [189.2934,   5.6882, -13.7727,  ...,   5.1959,   1.7971,  12.9250],\n",
       "          [307.2738,   9.5731, -10.2756,  ...,   4.7372,   1.8196,   9.1250],\n",
       "          ...,\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
       " \n",
       "         [[363.0639,  13.6516, -16.0650,  ...,   4.7065,   1.8098,  12.9250],\n",
       "          [186.5503,   5.6851, -13.6582,  ...,   5.1959,   1.7971,  12.9250],\n",
       "          [305.2582,   9.5700,  -9.8810,  ...,   4.7372,   1.8196,   9.1250],\n",
       "          ...,\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
       " \n",
       "         [[359.8646,  13.8451, -15.9280,  ...,   4.7065,   1.8098,  12.9250],\n",
       "          [183.8175,   5.7187, -13.6698,  ...,   5.1959,   1.7971,  12.9250],\n",
       "          [303.2824,   9.5547,  -9.8765,  ...,   4.7372,   1.8196,   9.1250],\n",
       "          ...,\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
       "          [     nan,      nan,      nan,  ...,      nan,      nan,      nan]]]),\n",
       " 'lane_markers': tensor([ 7.2700, 10.9800,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "             nan,     nan]),\n",
       " 'boundary_lines': tensor([ 3.2200, 15.1200]),\n",
       " 'agent_mask': tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout_data[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 27.0900,  50.7800,  78.8900, 112.4550, 172.2250,  40.3900,  60.4650,\n",
       "         30.9850, 223.9500,  87.7250,  66.6500, 112.8450, 154.7500, 107.4150,\n",
       "        255.3000, 132.0350, 185.8650, 163.2550, 201.2500, 294.2700, 191.2100,\n",
       "        243.3400, 211.5400, 225.6150, 316.4600, 280.2050, 247.4550, 341.1450,\n",
       "        294.3150, 320.9800, 356.1300, 311.7950, 334.7300, 365.3650, 376.6300,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan,      nan,      nan,      nan,      nan,      nan,\n",
       "             nan,      nan])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs['time_dependent'][-1][...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs['agent_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 100, 2)) of distribution Normal(loc: torch.Size([1, 100, 2]), scale: torch.Size([1, 100, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[[nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan]]], grad_fn=<ViewBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_dependent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlane_markers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mboundary_lines\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[67], line 116\u001b[0m, in \u001b[0;36mPPOActor.get_action\u001b[1;34m(self, time_dep, lane_markers, boundaries, agent_mask)\u001b[0m\n\u001b[0;32m    114\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_std)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Create a normal distribution per vehicle slot.\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Sample actions using reparameterization (this allows for differentiable sampling).\u001b[39;00m\n\u001b[0;32m    118\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrsample()  \u001b[38;5;66;03m# shape (N, M, output_size)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Productivity\\Anaconda\\envs\\deep\\Lib\\site-packages\\torch\\distributions\\normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Productivity\\Anaconda\\envs\\deep\\Lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 100, 2)) of distribution Normal(loc: torch.Size([1, 100, 2]), scale: torch.Size([1, 100, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[[nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan],\n         [nan, nan]]], grad_fn=<ViewBackward0>)"
     ]
    }
   ],
   "source": [
    "actor.get_action(obs['time_dependent'], obs['lane_markers'], obs['boundary_lines'], obs['agent_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[173], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrollout_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m34\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "rollout_data[34][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4855, -2.8273, -2.2401, -2.5778, -2.4378, -1.8631, -1.8576, -2.0488,\n",
       "        -2.8671, -2.3174, -2.7676, -4.3424, -1.8963, -2.0925, -3.1784, -3.7279,\n",
       "        -2.8142, -3.4707, -2.1737, -3.4811, -2.1687, -5.7620, -2.6282, -2.1297,\n",
       "        -2.7912, -5.1209, -1.8950, -2.3487, -2.2773, -1.8647, -2.2185, -2.8119,\n",
       "        -2.2143, -2.5488, -3.1125, -2.0399, -4.0850, -3.8517, -4.0146, -2.8722,\n",
       "        -2.6194, -2.1434, -2.1509, -2.5953, -2.4833, -3.5731, -2.3614, -3.6410,\n",
       "        -2.1411, -1.9383, -2.3136, -3.3060, -3.8306, -2.2841, -2.1443, -1.8807,\n",
       "        -2.3448, -3.4627, -3.1537, -1.9227, -2.2948, -2.8104, -2.1649, -2.8447,\n",
       "        -3.6454, -1.8410, -4.7004, -3.4866, -2.0449, -2.4044, -2.2190, -3.1293,\n",
       "        -3.4302, -2.6643, -3.8905, -4.3101, -6.5863, -3.2350, -2.4898, -2.6519,\n",
       "        -1.9388, -3.7525, -2.2636, -2.1696, -2.3912, -3.1848, -5.9453, -2.2931,\n",
       "        -2.3512, -6.4200, -2.8845, -3.5569, -1.9417, -2.1279, -4.1467, -2.1907,\n",
       "        -2.2525, -2.2613, -2.5749, -2.7236])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout_data[-1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = PPOActor(50, 100, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dep, lane_markers, boundaries = obs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1000 [00:00<00:16, 60.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/1000, Loss: 0.7866\n",
      "Iteration 20/1000, Loss: 0.7771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 38/1000 [00:00<00:16, 58.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30/1000, Loss: 0.7641\n",
      "Iteration 40/1000, Loss: 0.7755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 63/1000 [00:01<00:15, 58.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50/1000, Loss: 0.7645\n",
      "Iteration 60/1000, Loss: 0.7536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 81/1000 [00:01<00:15, 58.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70/1000, Loss: 0.7524\n",
      "Iteration 80/1000, Loss: 0.7307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:01<00:15, 58.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90/1000, Loss: 0.7258\n",
      "Iteration 100/1000, Loss: 0.7249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 118/1000 [00:02<00:15, 57.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 110/1000, Loss: 0.7112\n",
      "Iteration 120/1000, Loss: 0.7127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 142/1000 [00:02<00:14, 58.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 130/1000, Loss: 0.7019\n",
      "Iteration 140/1000, Loss: 0.6957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 160/1000 [00:02<00:14, 57.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 150/1000, Loss: 0.6891\n",
      "Iteration 160/1000, Loss: 0.6854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 178/1000 [00:03<00:14, 57.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 170/1000, Loss: 0.6790\n",
      "Iteration 180/1000, Loss: 0.6705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 196/1000 [00:03<00:14, 57.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190/1000, Loss: 0.6658\n",
      "Iteration 200/1000, Loss: 0.6657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 215/1000 [00:03<00:13, 57.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 210/1000, Loss: 0.6478\n",
      "Iteration 220/1000, Loss: 0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 240/1000 [00:04<00:12, 59.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 230/1000, Loss: 0.6370\n",
      "Iteration 240/1000, Loss: 0.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 259/1000 [00:04<00:12, 59.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 250/1000, Loss: 0.6264\n",
      "Iteration 260/1000, Loss: 0.6292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 283/1000 [00:04<00:12, 58.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 270/1000, Loss: 0.6203\n",
      "Iteration 280/1000, Loss: 0.6084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 296/1000 [00:05<00:11, 58.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 290/1000, Loss: 0.6010\n",
      "Iteration 300/1000, Loss: 0.6053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 316/1000 [00:05<00:11, 59.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 310/1000, Loss: 0.5939\n",
      "Iteration 320/1000, Loss: 0.5909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 340/1000 [00:05<00:11, 57.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 330/1000, Loss: 0.5861\n",
      "Iteration 340/1000, Loss: 0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 358/1000 [00:06<00:11, 57.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 350/1000, Loss: 0.5730\n",
      "Iteration 360/1000, Loss: 0.5726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 382/1000 [00:06<00:10, 58.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 370/1000, Loss: 0.5686\n",
      "Iteration 380/1000, Loss: 0.5560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [00:06<00:10, 58.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 390/1000, Loss: 0.5554\n",
      "Iteration 400/1000, Loss: 0.5557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 418/1000 [00:07<00:10, 56.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 410/1000, Loss: 0.5499\n",
      "Iteration 420/1000, Loss: 0.5441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 436/1000 [00:07<00:09, 57.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 430/1000, Loss: 0.5428\n",
      "Iteration 440/1000, Loss: 0.5305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 460/1000 [00:07<00:09, 57.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 450/1000, Loss: 0.5323\n",
      "Iteration 460/1000, Loss: 0.5319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 479/1000 [00:08<00:08, 58.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 470/1000, Loss: 0.5218\n",
      "Iteration 480/1000, Loss: 0.5175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 497/1000 [00:08<00:08, 58.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 490/1000, Loss: 0.5127\n",
      "Iteration 500/1000, Loss: 0.5110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 516/1000 [00:08<00:08, 59.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 510/1000, Loss: 0.5076\n",
      "Iteration 520/1000, Loss: 0.5038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 535/1000 [00:09<00:07, 59.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 530/1000, Loss: 0.4944\n",
      "Iteration 540/1000, Loss: 0.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 559/1000 [00:09<00:07, 58.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 550/1000, Loss: 0.4950\n",
      "Iteration 560/1000, Loss: 0.4836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 578/1000 [00:09<00:07, 58.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 570/1000, Loss: 0.4817\n",
      "Iteration 580/1000, Loss: 0.4744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 596/1000 [00:10<00:06, 58.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 590/1000, Loss: 0.4725\n",
      "Iteration 600/1000, Loss: 0.4676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 615/1000 [00:10<00:06, 58.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 610/1000, Loss: 0.4691\n",
      "Iteration 620/1000, Loss: 0.4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 640/1000 [00:11<00:06, 58.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 630/1000, Loss: 0.4635\n",
      "Iteration 640/1000, Loss: 0.4498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 658/1000 [00:11<00:06, 56.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 650/1000, Loss: 0.4526\n",
      "Iteration 660/1000, Loss: 0.4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 676/1000 [00:11<00:05, 57.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 670/1000, Loss: 0.4426\n",
      "Iteration 680/1000, Loss: 0.4435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 700/1000 [00:12<00:05, 57.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 690/1000, Loss: 0.4412\n",
      "Iteration 700/1000, Loss: 0.4385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 718/1000 [00:12<00:04, 57.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 710/1000, Loss: 0.4330\n",
      "Iteration 720/1000, Loss: 0.4279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 742/1000 [00:12<00:04, 57.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 730/1000, Loss: 0.4317\n",
      "Iteration 740/1000, Loss: 0.4246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 761/1000 [00:13<00:04, 57.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 750/1000, Loss: 0.4170\n",
      "Iteration 760/1000, Loss: 0.4091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 779/1000 [00:13<00:03, 56.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 770/1000, Loss: 0.4118\n",
      "Iteration 780/1000, Loss: 0.4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 797/1000 [00:13<00:03, 55.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 790/1000, Loss: 0.4068\n",
      "Iteration 800/1000, Loss: 0.4030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 815/1000 [00:14<00:03, 55.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 810/1000, Loss: 0.4026\n",
      "Iteration 820/1000, Loss: 0.3949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 839/1000 [00:14<00:02, 56.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 830/1000, Loss: 0.3896\n",
      "Iteration 840/1000, Loss: 0.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 857/1000 [00:14<00:02, 56.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 850/1000, Loss: 0.3859\n",
      "Iteration 860/1000, Loss: 0.3863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 875/1000 [00:15<00:02, 56.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 870/1000, Loss: 0.3897\n",
      "Iteration 880/1000, Loss: 0.3806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 899/1000 [00:15<00:01, 54.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 890/1000, Loss: 0.3750\n",
      "Iteration 900/1000, Loss: 0.3714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 917/1000 [00:15<00:01, 55.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 910/1000, Loss: 0.3772\n",
      "Iteration 920/1000, Loss: 0.3711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 941/1000 [00:16<00:01, 55.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 930/1000, Loss: 0.3661\n",
      "Iteration 940/1000, Loss: 0.3634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 959/1000 [00:16<00:00, 56.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 950/1000, Loss: 0.3622\n",
      "Iteration 960/1000, Loss: 0.3586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 977/1000 [00:17<00:00, 56.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 970/1000, Loss: 0.3560\n",
      "Iteration 980/1000, Loss: 0.3540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:17<00:00, 57.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 990/1000, Loss: 0.3478\n",
      "Iteration 1000/1000, Loss: 0.3475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "actor.train()  # set to training mode\n",
    "\n",
    "# Define an optimizer.\n",
    "optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop: We'll do a few iterations updating the network using a fake loss defined as the mean absolute acceleration.\n",
    "num_iterations = 1000\n",
    "for i in trange(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass.\n",
    "    accel_output, log_prob = actor.get_action(time_dep, lane_markers, boundaries)  # shape (N, M, 2)\n",
    "    # Compute a fake loss: mean of the absolute acceleration values.\n",
    "    loss = torch.abs(accel_output).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Iteration {i+1}/{num_iterations}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
